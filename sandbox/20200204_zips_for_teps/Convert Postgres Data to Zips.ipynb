{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert Postgres Data to Zips\n",
    "\n",
    "This notebook hacks `traffic_prophet` to convert the 15-minute bin volume data table `czhu.btp_centreline_volumes` into zip files of text files organized by centreline ID, direction and year.\n",
    "\n",
    "Each text file is tab-delimited with the columns:\n",
    "\n",
    "```\n",
    "nonsense\tcentreline_id\tdir_bin\tcount_bin\tvolume\tcount_type\n",
    "1921:43951732\t8237152\t-1\t28-Jan-2016 00:00:00\t50\t1\n",
    "1922:43951733\t8237152\t-1\t28-Jan-2016 00:15:00\t33\t1\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append('./bdit_traffic_prophet/')\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import knowyourdata as kyd\n",
    "import zipfile\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "from traffic_prophet import cfg\n",
    "import pathlib, os\n",
    "import configparser\n",
    "\n",
    "from traffic_prophet import connection\n",
    "from traffic_prophet.countmatch import reader\n",
    "\n",
    "defaultcolours = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "\n",
    "filepath = pathlib.Path.home().joinpath('.charlesconfig')\n",
    "if os.path.isfile(filepath):\n",
    "    vol_conn = connection.Connection(filepath, 'POSTGRES',\n",
    "                                     'czhu.btp_centreline_volumes')\n",
    "    ll_conn = connection.Connection(filepath, 'POSTGRES',\n",
    "                                    'czhu.btp_centreline_lonlat')\n",
    "    config = configparser.RawConfigParser()\n",
    "    config.read(filepath.as_posix())\n",
    "    MAPBOX_TOKEN = config['MAPBOX']['token']\n",
    "    PLOTLY_USER = config['PLOTLY']['user']\n",
    "    PLOTLY_KEY = config['PLOTLY']['key']\n",
    "else:\n",
    "    raise ValueError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from traffic_prophet.countmatch import reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get HW401 centreline IDs\n",
    "zipsn = [\"../../VolumeModel/TEPS-EEDrun/PRTCS/negative/15min_counts_{0}.zip\".format(x)\n",
    "         for x in range(2006, 2017)]\n",
    "zipsp = [\"../../VolumeModel/TEPS-EEDrun/PRTCS/positive/15min_counts_{0}.zip\".format(x)\n",
    "         for x in range(2006, 2017)]\n",
    "zips = zipsn + zipsp\n",
    "rdr = reader.ReaderZip(zips)\n",
    "\n",
    "re_centrelines = []\n",
    "\n",
    "for zf in rdr.source:\n",
    "    for c in rdr.get_zipreader(zf):\n",
    "        if 're' in c['filename']:\n",
    "            re_centrelines.append([c['filename'], c['centreline_id']])\n",
    "\n",
    "re_centreline_ids = list(set([x[1] for x in re_centrelines]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all centreline IDs in mid_f_point.csv\n",
    "df = pd.read_csv(\"../../VolumeModel/TEPS-EEDrun/PRTCS/negative/mid_f_point.csv\", header=None)\n",
    "midpoint_centerline_ids = list(df[0].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all centreline IDs in Landuse_pop_lane_speed.xlsx\n",
    "df = pd.read_excel(\"../../VolumeModel/TEPS-EEDrun/PRTCS/negative/locals/Landuse_pop_lane_speed.xlsx\")\n",
    "landuse_centreline_ids = list(df['centreline'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "available_teps_centreline_ids = list(\n",
    "    set(midpoint_centerline_ids).intersection(set(landuse_centreline_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReaderPostgresRaw(reader.ReaderBase):\n",
    "    \"\"\"Hacked method of accessing the raw 15-minute bin table from Postgres.\"\"\"\n",
    "\n",
    "    def get_pgreader(self, year):\n",
    "        with self.source.connect() as db_con:\n",
    "            sql_cmd = (\n",
    "                (\"SELECT centreline_id, dir_bin, count_bin, volume, count_type \"\n",
    "                 \"FROM {dbt} WHERE EXTRACT(year from count_bin) = {year} \"\n",
    "                 \"ORDER BY centreline_id, dir_bin, count_bin\")\n",
    "                .format(dbt=self.source.tablename,  year=year))\n",
    "\n",
    "            all_data = pd.read_sql(sql_cmd, db_con,\n",
    "                                   parse_dates=['count_bin', ])\n",
    "\n",
    "            for key, df in all_data.groupby(['centreline_id', 'dir_bin']):\n",
    "                centreline_id = key[0]\n",
    "                direction = key[1]\n",
    "\n",
    "                data = df[['count_bin', 'volume', 'count_type']].copy()\n",
    "                data.columns = ['Timestamp', '15-minute Volume', 'Count Type']\n",
    "                data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "                # Filename is used to flag for HW401 data in Arman's zip files,\n",
    "                # so just pass a dummy value here.  Note that we can't use\n",
    "                # 'postgres' here since it contains 're'!\n",
    "                yield {'filename': 'fromPG',\n",
    "                       'centreline_id': int(centreline_id),\n",
    "                       'direction': int(direction),\n",
    "                       'data': data,\n",
    "                       'year': year}\n",
    "\n",
    "    def write_db_to_zip(self, year, fpath=\"./\"):\n",
    "        \"\"\"Writes a year's worth of 15 minute bins\"\"\"\n",
    "        rdr = self.get_pgreader(year)\n",
    "\n",
    "        fhzp = zipfile.ZipFile(\n",
    "            fpath + \"15min_counts_{0}_positive.zip\".format(year), 'w')\n",
    "        fhzn = zipfile.ZipFile(\n",
    "            fpath + \"15min_counts_{0}_negative.zip\".format(year), 'w')\n",
    "\n",
    "        for tc in rdr:\n",
    "            # Control sequence to prevent centreline_ids on HW401 and those with\n",
    "            # no land use data from being included in zip.\n",
    "            if tc['centreline_id'] in re_centreline_ids:\n",
    "                warnings.warn(\"{0} found in HW401 IDs!\".format(tc['centreline_id']))\n",
    "                continue\n",
    "            elif tc['centreline_id'] not in available_teps_centreline_ids:\n",
    "                warnings.warn(\"{0} doesn't have TEPs land use/geographic data!\".format(tc['centreline_id']))\n",
    "                continue\n",
    "\n",
    "            # Extract data from dict and convert it to TEPs format.\n",
    "            data = tc['data']\n",
    "            # Convert to DD-MMM-YYYY TT:TT:TT format favoured by Matlab.\n",
    "            data['Timestamp'] = data['Timestamp'].dt.strftime(r\"%d-%b-%Y %T\")\n",
    "            data['Nonsense'] = '999:9999999'\n",
    "            data['Centreline ID'] = tc['centreline_id']\n",
    "            data['Direction'] = tc['direction']\n",
    "            # Output to csv, but dump to string instead of file.\n",
    "            datastr = data[['Nonsense', 'Centreline ID', 'Direction',\n",
    "                            'Timestamp', '15-minute Volume', 'Count Type']].to_csv(\n",
    "                None, sep='\\t', na_rep='N/A', header=False, index=False)\n",
    "\n",
    "            filename = \"{0}_99999_{1}.txt\".format(tc['centreline_id'], year)\n",
    "            if tc['direction'] > 0:\n",
    "                fhzp.writestr(filename, datastr)\n",
    "            else:\n",
    "                fhzn.writestr(filename, datastr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgreader = ReaderPostgresRaw(vol_conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgreader.write_db_to_zip(2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgreader.write_db_to_zip(2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
